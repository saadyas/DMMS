{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saadyas/DMMS/blob/main/DMMS-modifications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CVOJpBOkiTkd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVOJpBOkiTkd",
        "outputId": "243a3501-21fa-4125-a776-03670e13f527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fA2pFtYflvSy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fA2pFtYflvSy",
        "outputId": "209ca208-af51-46f2-c4ef-62cdfb7d0091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3cfdcc-c0e9-4383-83f4-49c715d62556",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee3cfdcc-c0e9-4383-83f4-49c715d62556",
        "outputId": "59756b8c-cc04-4c21-b526-34304405e5ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f84fe0-7a14-4897-8f99-f28a2a906971",
      "metadata": {
        "id": "68f84fe0-7a14-4897-8f99-f28a2a906971"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import h5py\n",
        "import numpy as np\n",
        "import json\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MSMODataset(object):\n",
        "    def __init__(self, mode='train', args=None):\n",
        "        self.gt = json.load(open('{}/{}/annotation/{}.json'.format(args.get('data_root'), args.get('dataset'), mode)))\n",
        "        self.id_list = list(self.gt.keys())\n",
        "\n",
        "        self.video_dict = np.load('{}/{}/feature/video_resnet50_{}.npy'.format(args.get('data_root'), args.get('dataset'), mode), allow_pickle=True).item()\n",
        "        self.text_dict = np.load('{}/{}/feature/text_roberta_{}.npy'.format(args.get('data_root'), args.get('dataset'), mode), allow_pickle=True).item()\n",
        "        if args.get('dataset') == 'Daily_Mail':\n",
        "            self.video_summ_dict = np.load('{}/{}/feature/video_summ_resnet50_{}.npy'.format(args.get('data_root'), args.get('dataset'), mode), allow_pickle=True).item()\n",
        "        else:\n",
        "            self.video_summ_dict = {}\n",
        "\n",
        "        for id in tqdm(self.id_list):\n",
        "            self.video_dict[id] = torch.tensor(self.video_dict[id]).to(torch.float32)\n",
        "            self.text_dict[id] = torch.tensor(self.text_dict[id]).to(torch.float32)\n",
        "\n",
        "            if args.get('dataset') == 'Daily_Mail':\n",
        "                self.video_summ_dict[id] = torch.tensor(self.video_summ_dict[id]).to(torch.float32)\n",
        "            else:\n",
        "                self.video_summ_dict[id] = torch.zeros(1).to(torch.float32)\n",
        "        self.dataset = args.get('dataset')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        id = self.id_list[index]\n",
        "\n",
        "        video = self.video_dict[id] # [T, 2048]\n",
        "        video_summ = self.video_summ_dict[id]\n",
        "        text = self.text_dict[id] # [N, 768]\n",
        "        #print('\\nID : {}'.format(id))\n",
        "        num_frame = video.shape[0]\n",
        "        num_keyframe = video_summ.shape[0]\n",
        "        num_sentence = text.shape[0]\n",
        "\n",
        "        if self.dataset == 'Daily_Mail':\n",
        "            video_label = torch.tensor(self.gt[id]['video_label'], dtype=torch.long)\n",
        "            assert torch.sum(video_label) == num_keyframe\n",
        "        else:\n",
        "            video_label = torch.zeros(num_frame).to(torch.long)\n",
        "            video_label[0] = 1\n",
        "        text_label = torch.tensor(self.gt[id]['text_label'], dtype=torch.long)\n",
        "\n",
        "        article_sentence = self.gt[id]['article_sentence']\n",
        "        highlight = self.gt[id]['highlight']\n",
        "\n",
        "        mask_video = torch.ones(num_frame, dtype=torch.long)\n",
        "        mask_video_summ = torch.ones(num_keyframe, dtype=torch.long)\n",
        "        mask_text = torch.ones(num_sentence, dtype=torch.long)\n",
        "\n",
        "        video_to_text_mask = torch.zeros(1)\n",
        "        text_to_video_mask = torch.zeros(1)\n",
        "        return video, video_summ, text, mask_video, mask_video_summ, mask_text, video_label, text_label, article_sentence, highlight, video_to_text_mask, text_to_video_mask\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    \"\"\"\n",
        "    Re-seed each worker process to preserve reproducibility\n",
        "    \"\"\"\n",
        "    worker_seed = torch.initial_seed() % 2**32\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)\n",
        "    return\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    batched_output_list = []\n",
        "    for i in range(len(batch[0])):\n",
        "        batched_output = [item[i] for item in batch]\n",
        "        batched_output_list.append(batched_output)\n",
        "    return batched_output_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492bc0e2-3b55-4564-a624-4c77ffa0cda0",
      "metadata": {
        "id": "492bc0e2-3b55-4564-a624-4c77ffa0cda0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import random\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from os import PathLike\n",
        "from typing import Any, List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_random_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "def init_logger(log_dir: str, log_file: str) -> None:\n",
        "    logger = logging.getLogger()\n",
        "    format_str = r'[%(asctime)s] %(message)s'\n",
        "    logging.basicConfig(\n",
        "        level=logging.INFO,\n",
        "        datefmt=r'%Y/%m/%d %H:%M:%S',\n",
        "        format=format_str\n",
        "    )\n",
        "    log_dir = Path(log_dir)\n",
        "    log_dir.mkdir(parents=True, exist_ok=True)\n",
        "    fh = logging.FileHandler(str(log_dir / log_file))\n",
        "    fh.setFormatter(logging.Formatter(format_str))\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "def load_yaml(path: PathLike) -> Any:\n",
        "    with open(path) as f:\n",
        "        obj = yaml.safe_load(f)\n",
        "    return obj\n",
        "\n",
        "def dump_yaml(obj: Any, path: PathLike) -> None:\n",
        "    with open(path, 'w') as f:\n",
        "        yaml.dump(obj, f)\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self, *keys: str):\n",
        "        self.totals = {key: 0.0 for key in keys}\n",
        "        self.counts = {key: 0 for key in keys}\n",
        "\n",
        "    def update(self, **kwargs: float) -> None:\n",
        "        for key, value in kwargs.items():\n",
        "            self._check_attr(key)\n",
        "            self.totals[key] += value\n",
        "            self.counts[key] += 1\n",
        "\n",
        "    def __getattr__(self, attr: str) -> float:\n",
        "        self._check_attr(attr)\n",
        "        total = self.totals[attr]\n",
        "        count = self.counts[attr]\n",
        "        return total / count if count else 0.0\n",
        "\n",
        "    def _check_attr(self, attr: str) -> None:\n",
        "        assert attr in self.totals and attr in self.counts\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ImprovedContrastiveLoss(nn.Module):\n",
        "    def __init__(self, temperature=0.07):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1.0 / self.temperature)))\n",
        "\n",
        "    def forward(self, q, k, negatives):\n",
        "        \"\"\"\n",
        "        q: Query embeddings (B, D)\n",
        "        k: Positive key embeddings (B, D)\n",
        "        negatives: Negative samples (B, N, D) or (B, D)\n",
        "        \"\"\"\n",
        "        q = q.squeeze(1)  # Ensure 2D shape (B, D)\n",
        "        k = k.squeeze(1)  # Ensure 2D shape (B, D)\n",
        "\n",
        "        B, D = q.shape  # Batch size (B) and feature size (D)\n",
        "\n",
        "        # Ensure negatives are properly shaped\n",
        "        if negatives.dim() == 2:\n",
        "            negatives = negatives.unsqueeze(1)  # Convert (B, D) → (B, 1, D)\n",
        "        elif negatives.dim() == 3 and negatives.shape[1] != 1:\n",
        "            negatives = negatives.view(B, -1, D)  # Ensure correct shape (B, N, D)\n",
        "\n",
        "        N = negatives.shape[1]  # Number of negative samples per batch\n",
        "\n",
        "        # Normalize embeddings with epsilon to prevent NaN errors\n",
        "        eps = 1e-6\n",
        "        q = F.normalize(q, dim=-1, eps=eps)  # (B, D)\n",
        "        k = F.normalize(k, dim=-1, eps=eps)  # (B, D)\n",
        "        negatives = F.normalize(negatives, dim=-1, eps=eps)  # (B, N, D)\n",
        "\n",
        "        # Reshape positive keys `k` for batch matrix multiplication\n",
        "        k = k.view(B, D, 1)  # (B, D, 1)\n",
        "\n",
        "        # Compute positive similarity: (B, D) x (B, D, 1) → (B, 1)\n",
        "        pos_sim = torch.bmm(q.unsqueeze(1), k).squeeze(1) / self.temperature  # (B, 1)\n",
        "\n",
        "        # 🛠 **Fixed negative similarity calculation**\n",
        "        # Ensure negatives are properly shaped before bmm()\n",
        "        negatives = negatives.view(B, -1, D)  # (B, N, D)\n",
        "\n",
        "        # Compute negative similarity: (B, 1, D) x (B, D, N) → (B, 1, N)\n",
        "        neg_sim = torch.bmm(q.unsqueeze(1), negatives.permute(0, 2, 1)).squeeze(1) / self.temperature  # (B, N)\n",
        "\n",
        "        # Ensure shape consistency before concatenation\n",
        "        logits = torch.cat([pos_sim, neg_sim], dim=1)  # (B, 1 + N)\n",
        "\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(q.device)  # Target index for positives (first column)\n",
        "\n",
        "        # Compute contrastive loss\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "MEA5EPkdYrHJ"
      },
      "id": "MEA5EPkdYrHJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41f883b-2e55-43e0-bd8e-169ce6c53acc",
      "metadata": {
        "id": "a41f883b-2e55-43e0-bd8e-169ce6c53acc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "def calc_cls_loss(pred: torch.Tensor,\n",
        "                  target: torch.Tensor,\n",
        "                  mask: torch.Tensor = None,\n",
        "                  ) -> torch.Tensor:\n",
        "    \"\"\"Compute classification loss on both positive and negative samples.\n",
        "\n",
        "    :param pred: Predicted class. Sized [B, N].\n",
        "    :param target: Class target where 1 marks positive, and 0\n",
        "        marks ignored. Sized [B, N].\n",
        "    :param kind: Loss type. Choose from (focal, cross-entropy).\n",
        "    :param mask: indicts the valid segments for each video\n",
        "    :return: Scalar loss value.\n",
        "    \"\"\"\n",
        "\n",
        "    pred = torch.sigmoid(pred)\n",
        "    pred = torch.stack([1 - pred, pred], dim=-1)\n",
        "    mask = mask.to(torch.bool)\n",
        "    loss = focal_loss(pred, target, reduction='none')\n",
        "    loss = loss[mask, :]\n",
        "    loss = torch.mean(loss)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def focal_loss(pred: torch.Tensor,\n",
        "               target: torch.Tensor,\n",
        "               alpha: float = 0.25,\n",
        "               gamma: float = 2,\n",
        "               reduction: str = 'sum'\n",
        "               ) -> torch.Tensor:\n",
        "    \"\"\"Compute focal loss for binary classification.\n",
        "        FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
        "\n",
        "    :param pred: Predicted confidence. Sized [B, N, D].\n",
        "    :param target: Ground truth target. Sized [B, N].\n",
        "    :param alpha: Alpha parameter in focal loss.\n",
        "    :param gamma: Gamma parameter in focal loss.\n",
        "    :param reduction: Aggregation type. Choose from (sum, mean, none).\n",
        "    :return: Scalar loss value.\n",
        "    \"\"\"\n",
        "    B, _, num_classes = pred.shape\n",
        "    t = F.one_hot(target, num_classes)\n",
        "\n",
        "    p_t = pred * t + (1 - pred) * (1 - t)\n",
        "    alpha_t = alpha * t + (1 - alpha) * (1 - t)\n",
        "    fl = -alpha_t * (1 - p_t).pow(gamma) * p_t.clamp(min=1e-7).log()\n",
        "\n",
        "    ## TODO: update the sum to mean aross the batch axis\n",
        "    if reduction == 'sum':\n",
        "        fl = fl.sum()\n",
        "    elif reduction == 'mean':\n",
        "        fl = fl.mean()\n",
        "    elif reduction == 'none':\n",
        "        pass\n",
        "    else:\n",
        "        raise ValueError(f'Invalid reduction mode {reduction}')\n",
        "\n",
        "    return fl\n",
        "\n",
        "\n",
        "def iou_offset(offset_a: torch.Tensor,\n",
        "               offset_b: torch.Tensor,\n",
        "               eps: float = 1e-8\n",
        "               ) -> torch.Tensor:\n",
        "    \"\"\"Compute IoU offsets between multiple offset pairs.\n",
        "\n",
        "    :param offset_a: Offsets of N positions. Sized [N, 2].\n",
        "    :param offset_b: Offsets of N positions. Sized [N, 2].\n",
        "    :param eps: Small floating value to prevent division by zero.\n",
        "    :return: IoU values of N positions. Sized [N].\n",
        "    \"\"\"\n",
        "    left_a, right_a = offset_a[:, 0], offset_a[:, 1]\n",
        "    left_b, right_b = offset_b[:, 0], offset_b[:, 1]\n",
        "\n",
        "    length_a = left_a + right_a\n",
        "    length_b = left_b + right_b\n",
        "\n",
        "    intersect = torch.min(left_a, left_b) + torch.min(right_a, right_b)\n",
        "    intersect[intersect < 0] = 0\n",
        "    union = length_a + length_b - intersect\n",
        "    union[union <= 0] = eps\n",
        "\n",
        "    iou = intersect / union\n",
        "    return iou\n",
        "\n",
        "\n",
        "def calc_loc_loss(pred_loc_batch: torch.Tensor,\n",
        "                  test_loc_batch: torch.Tensor,\n",
        "                  cls_label: torch.Tensor,\n",
        "                  kind: str = 'soft-iou',\n",
        "                  eps: float = 1e-8\n",
        "                  ) -> torch.Tensor:\n",
        "    \"\"\"Compute soft IoU loss for regression only on positive samples.\n",
        "\n",
        "    :param pred_loc_batch: Predicted offsets. Sized [B, N, 2].\n",
        "    :param test_loc_batch: Ground truth offsets. Sized [B, N, 2].\n",
        "    :param cls_label: Class label specifying positive samples.\n",
        "    :param kind: Loss type. Choose from (soft-iou, smooth-l1).\n",
        "    :param eps: Small floating value to prevent division by zero.\n",
        "    :return: Scalar loss value.\n",
        "    \"\"\"\n",
        "    cls_label = cls_label.to(torch.bool)\n",
        "    batch_size = cls_label.shape[0]\n",
        "\n",
        "    loss_sum = 0\n",
        "    for i in range(batch_size):\n",
        "        pred_loc = pred_loc_batch[i, cls_label[i]]\n",
        "        test_loc = test_loc_batch[i, cls_label[i]]\n",
        "\n",
        "        if kind == 'soft-iou':\n",
        "            iou = iou_offset(pred_loc, test_loc)\n",
        "            loss = -torch.log(iou + eps).mean()\n",
        "        elif kind == 'smooth-l1':\n",
        "            loss = F.smooth_l1_loss(pred_loc, test_loc)\n",
        "        else:\n",
        "            raise ValueError(f'Invalid loss type {kind}')\n",
        "        loss_sum += loss\n",
        "\n",
        "    loss = loss_sum / batch_size\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_ctr_loss(pred_batch, test_batch, pos_mask):\n",
        "    pos_mask = pos_mask.to(torch.bool) #[B, T]\n",
        "    batch_size = pos_mask.shape[0]\n",
        "\n",
        "    loss_sum = 0\n",
        "    for i in range(batch_size):\n",
        "        pred = pred_batch[i, pos_mask[i]] #[M]\n",
        "        test = test_batch[i, pos_mask[i]] #[M]\n",
        "        loss = F.binary_cross_entropy(pred, test)\n",
        "        loss_sum += loss\n",
        "\n",
        "    loss = loss_sum / batch_size\n",
        "    return loss\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_text_rouge(article_sentence_list, highlight_list, selected_sentence_index_list, dataset=None, rouge=None):\n",
        "    batch_size = len(selected_sentence_index_list)\n",
        "\n",
        "    R1_sum = 0\n",
        "    R2_sum = 0\n",
        "    RL_sum = 0\n",
        "    for i in range(batch_size):\n",
        "        sorted_index_list = sorted(selected_sentence_index_list[i])\n",
        "        selected_sentence_list = []\n",
        "        for selected_sentence_index in sorted_index_list:\n",
        "            selected_sentence_list.append(article_sentence_list[i][selected_sentence_index])\n",
        "\n",
        "        evaluated_sentence = ' '.join(selected_sentence_list)\n",
        "        if isinstance(highlight_list[i], list):\n",
        "            reference_sentence = ' '.join(highlight_list[i])\n",
        "        elif isinstance(highlight_list[i], str):\n",
        "            reference_sentence = highlight_list[i]\n",
        "        scores = rouge.score(evaluated_sentence, reference_sentence)\n",
        "        R1_sum += scores['rouge1'][2]\n",
        "        R2_sum += scores['rouge2'][2]\n",
        "        RL_sum += scores['rougeLsum'][2]\n",
        "\n",
        "    R1_mean = R1_sum / batch_size\n",
        "    R2_mean = R2_sum / batch_size\n",
        "    RL_mean = RL_sum / batch_size\n",
        "    return R1_mean, R2_mean, RL_mean\n",
        "\n",
        "@torch.no_grad()\n",
        "def calc_video_cos(video, gt_summ, keyframe_index_list, mask_video_summ=None, dataset=None):\n",
        "    batch_size = len(keyframe_index_list)\n",
        "    gt_summ = F.normalize(gt_summ, dim=-1)\n",
        "\n",
        "    cos_sim_sum = 0\n",
        "    for i in range(batch_size):\n",
        "        if dataset == 'Daily_Mail':\n",
        "            pred_summ = video[i][keyframe_index_list[i]]\n",
        "            pred_summ = F.normalize(pred_summ, dim=1)\n",
        "            sim_mat = gt_summ[i, mask_video_summ[i]] @ pred_summ.permute(1, 0)\n",
        "            sim_mat = sim_mat.detach().cpu().numpy()\n",
        "        elif dataset == 'BLiSS':\n",
        "            pred_summ = F.normalize(video[i], dim=1)\n",
        "            sim_mat = gt_summ[i, mask_video_summ[i]] @ pred_summ.permute(1, 0)\n",
        "            sim_mat = sim_mat - torch.min(sim_mat)\n",
        "            sim_mat = sim_mat / torch.max(sim_mat).clamp(min=1e-6)\n",
        "            sim_mat = sim_mat[:, keyframe_index_list[i]]\n",
        "            sim_mat = sim_mat.detach().cpu().numpy()\n",
        "\n",
        "        # select the largest-K pairwise cosine simialrity (K = num_key_frame)\n",
        "        num_key_frame = len(keyframe_index_list[i])\n",
        "        match_mat = np.zeros((num_key_frame, num_key_frame), dtype=int)\n",
        "        sorted_index = np.dstack(np.unravel_index(np.argsort(-sim_mat.ravel()), sim_mat.shape))[0] #[N*N, 2]\n",
        "        select_key_frame_count = 0\n",
        "        for j in range(sorted_index.shape[0]):\n",
        "            m, n = sorted_index[j]\n",
        "            if not match_mat[m, :].any() and not match_mat[:, n].any():\n",
        "                match_mat[m, n] = 1\n",
        "                select_key_frame_count += 1\n",
        "            if select_key_frame_count >= num_key_frame:\n",
        "                break\n",
        "\n",
        "        cos_sim = np.sum(sim_mat * match_mat) / np.sum(match_mat)\n",
        "        cos_sim_sum += cos_sim\n",
        "\n",
        "    cos_sim_mean = cos_sim_sum / batch_size\n",
        "    return cos_sim_mean\n",
        "\n",
        "\n",
        "class NCE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NCE, self).__init__()\n",
        "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
        "\n",
        "    def forward(self, q, k, neg, device='cuda:0'):\n",
        "        q = F.normalize(q, dim=1) #[1, C]\n",
        "        k = F.normalize(k, dim=1) #[1, C]\n",
        "        neg = F.normalize(neg, dim=1) #[T, C]\n",
        "        l_pos = q @ k.T #[1, 1]\n",
        "        l_neg = q @ neg.T #[1, T]\n",
        "        logits = torch.cat([l_pos, l_neg], dim=1) #[1, 1 + T]\n",
        "        logits *= self.logit_scale #[1, 1 + T]\n",
        "\n",
        "        labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Dual_Contrastive_Loss(nn.Module):\n",
        "    def __init__(self, args=None):\n",
        "        super().__init__()\n",
        "        self.video_contrast = ImprovedContrastiveLoss()\n",
        "        self.text_contrast = ImprovedContrastiveLoss()\n",
        "\n",
        "    def forward(self, contrastive_pairs):\n",
        "        if len(contrastive_pairs) == 0:\n",
        "            return torch.zeros(1).cuda(), torch.zeros(1).cuda()\n",
        "\n",
        "        cls_video = contrastive_pairs['cls_video']  # (B, D)\n",
        "        cls_text = contrastive_pairs['cls_text']  # (B, D)\n",
        "        key_video_list = contrastive_pairs['key_video_list']\n",
        "        nonkey_video_list = contrastive_pairs['nonkey_video_list']\n",
        "        key_text_list = contrastive_pairs['key_text_list']\n",
        "        nonkey_text_list = contrastive_pairs['nonkey_text_list']\n",
        "\n",
        "        B = cls_video.shape[0]\n",
        "        device = cls_video.device\n",
        "\n",
        "# Pad the nonkey_video_list and nonkey_text_list to have the same shape\n",
        "        nonkey_video_list = rnn_utils.pad_sequence(nonkey_video_list, batch_first=True, padding_value=0)\n",
        "        nonkey_text_list = rnn_utils.pad_sequence(nonkey_text_list, batch_first=True, padding_value=0)\n",
        "        # Compute inter-sample contrastive loss (video <-> text alignment)\n",
        "        # Remove the unnecessary torch.stack call\n",
        "        inter_video_loss = self.video_contrast(cls_video, cls_text, nonkey_video_list)\n",
        "        inter_text_loss = self.text_contrast(cls_text, cls_video, nonkey_text_list)\n",
        "        inter_contrastive_loss = (inter_video_loss + inter_text_loss) / 2\n",
        "\n",
        "\n",
        "        # Compute intra-sample contrastive loss (key vs non-key embeddings)\n",
        "        intra_contrastive_loss = 0\n",
        "        for i in range(B):\n",
        "            intra_video_loss = self.video_contrast(\n",
        "                torch.mean(key_video_list[i], dim=0, keepdim=True),\n",
        "                torch.mean(key_text_list[i], dim=0, keepdim=True),\n",
        "                nonkey_video_list[i]\n",
        "            )\n",
        "            intra_text_loss = self.text_contrast(\n",
        "                torch.mean(key_text_list[i], dim=0, keepdim=True),\n",
        "                torch.mean(key_video_list[i], dim=0, keepdim=True),\n",
        "                nonkey_text_list[i]\n",
        "            )\n",
        "            intra_contrastive_loss += (intra_video_loss + intra_text_loss) / 2\n",
        "\n",
        "        intra_contrastive_loss /= B\n",
        "        return inter_contrastive_loss, intra_contrastive_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefc9f91-644d-41c9-80d3-cf7f7f960d04",
      "metadata": {
        "id": "eefc9f91-644d-41c9-80d3-cf7f7f960d04"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "\n",
        "class MoELayer(nn.Module):\n",
        "    \"\"\"Mixture of Experts Layer with Load Balancing Loss\"\"\"\n",
        "    def __init__(self, num_experts, hidden_size, dropout=0.1, ratio=4):\n",
        "        super().__init__()\n",
        "        self.experts = nn.ModuleList([\n",
        "            FFN(hidden_size, p=dropout, ratio=ratio) for _ in range(num_experts)\n",
        "        ])\n",
        "        self.gate = nn.Linear(hidden_size, num_experts)\n",
        "        self.num_experts = num_experts\n",
        "        self.balancing_coef = 0.01  # For load balancing loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Gating weights [B, N, num_experts]\n",
        "        gates = torch.softmax(self.gate(x), dim=-1)\n",
        "\n",
        "        # Expert outputs [B, N, num_experts, C]\n",
        "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
        "\n",
        "        # Weighted sum [B, N, C]\n",
        "        out = torch.einsum('bnec,bne->bnc', expert_outputs, gates)\n",
        "\n",
        "        # Load balancing loss (auxiliary loss)\n",
        "        mask = (x.abs().sum(dim=-1) > 0)  # Ignore padding\n",
        "        importance = gates[mask].sum(dim=0)  # [num_experts]\n",
        "        loss = (importance.var() / (importance.mean()**2 + 1e-10)) * self.balancing_coef\n",
        "\n",
        "        return out, loss\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self,\n",
        "                 dims,\n",
        "                 k_dims=None,\n",
        "                 v_dims=None,\n",
        "                 h_dims=None,\n",
        "                 o_dims=None,\n",
        "                 heads=8,\n",
        "                 p=0.1,\n",
        "                 bias=True):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "\n",
        "        self._q_dims = dims\n",
        "        self._k_dims = k_dims or dims\n",
        "        self._v_dims = v_dims or dims\n",
        "        self._h_dims = h_dims or dims\n",
        "        self._o_dims = o_dims or dims\n",
        "        self._heads = heads\n",
        "        self._p = p\n",
        "        self._bias = bias\n",
        "        self._head_dims = self._h_dims // heads\n",
        "\n",
        "        self.q = nn.Linear(self._q_dims, self._h_dims, bias=bias)\n",
        "        self.k = nn.Linear(self._k_dims, self._h_dims, bias=bias)\n",
        "        self.v = nn.Linear(self._v_dims, self._h_dims, bias=bias)\n",
        "        self.m = nn.Linear(self._h_dims, self._o_dims, bias=bias)\n",
        "\n",
        "        self.drop1 = nn.Dropout(p)\n",
        "        self.drop2 = nn.Dropout(p)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return ('{}(q_dims={}, k_dims={}, v_dims={}, h_dims={}, o_dims={}, '\n",
        "                'heads={}, p={}, bias={})'.format(self.__class__.__name__,\n",
        "                                                  self._q_dims, self._k_dims,\n",
        "                                                  self._v_dims, self._h_dims,\n",
        "                                                  self._o_dims, self._heads,\n",
        "                                                  self._p, self._bias))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for m in (self.q, self.k, self.v, self.m):\n",
        "            nn.init.xavier_normal_(m.weight, gain=1.0)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, q, k=None, v=None, mask=None):\n",
        "        v = v if torch.is_tensor(v) else k if torch.is_tensor(k) else q\n",
        "        k = k if torch.is_tensor(k) else q\n",
        "\n",
        "        q = self.q(q).transpose(0, 1).contiguous()\n",
        "        k = self.k(k).transpose(0, 1).contiguous()\n",
        "        v = self.v(v).transpose(0, 1).contiguous()\n",
        "\n",
        "        b = q.size(1) * self._heads\n",
        "\n",
        "        q = q.view(-1, b, self._head_dims).transpose(0, 1)\n",
        "        k = k.view(-1, b, self._head_dims).transpose(0, 1)\n",
        "        v = v.view(-1, b, self._head_dims).transpose(0, 1)\n",
        "\n",
        "        att = torch.bmm(q, k.transpose(1, 2)) / self._head_dims**0.5\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = torch.where(mask > 0, .0, float('-inf'))\n",
        "            mask = mask.repeat_interleave(self._heads, dim=0)\n",
        "            att += mask\n",
        "\n",
        "        att = att.softmax(-1)\n",
        "\n",
        "        if self.drop1 is not None:\n",
        "            att = self.drop1(att)\n",
        "\n",
        "        m = torch.bmm(att, v).transpose(0, 1).contiguous()\n",
        "        m = m.view(m.size(0), -1, self._h_dims).transpose(0, 1)\n",
        "        m = self.m(m)\n",
        "\n",
        "        if self.drop2 is not None:\n",
        "            m = self.drop2(m)\n",
        "\n",
        "        return m\n",
        "\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, num_input, p=0.1, ratio=4):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(num_input, num_input * ratio)\n",
        "        self.act = nn.GELU()\n",
        "        self.drop1 = nn.Dropout(p)\n",
        "        self.fc2 = nn.Linear(num_input * ratio, num_input)\n",
        "        self.drop2 = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(x)\n",
        "        return x\n",
        "\n",
        "class MultiWayTransformer(nn.Module):\n",
        "    def __init__(self, num_hidden, dropout_attn=0.1, num_experts=4):\n",
        "        super().__init__()\n",
        "        self.norm1_fused = nn.LayerNorm(num_hidden)\n",
        "        self.attn_fusion = MultiHeadAttention(num_hidden, p=dropout_attn)\n",
        "\n",
        "        # Replace FFNs with MoE Layers\n",
        "        self.norm2_video = nn.LayerNorm(num_hidden)\n",
        "        self.moe_video = MoELayer(num_experts, num_hidden, dropout_attn)\n",
        "\n",
        "        self.norm2_text = nn.LayerNorm(num_hidden)\n",
        "        self.moe_text = MoELayer(num_experts, num_hidden, dropout_attn)\n",
        "\n",
        "        self.num_experts = num_experts\n",
        "\n",
        "    def forward(self, fused, mask_fused, N_video, N_text):\n",
        "        residual = fused\n",
        "\n",
        "        # Cross-modal attention\n",
        "        fused = self.norm1_fused(fused)\n",
        "        fused = self.attn_fusion(fused, fused, fused, mask=mask_fused)\n",
        "        residual = residual + fused\n",
        "\n",
        "        residual_video, residual_text = torch.split(residual, [N_video, N_text], dim=1)\n",
        "\n",
        "        # Video MoE branch\n",
        "        video = self.norm2_video(residual_video)\n",
        "        video, video_moe_loss = self.moe_video(video)\n",
        "        residual_video = residual_video + video\n",
        "\n",
        "        # Text MoE branch\n",
        "        text = self.norm2_text(residual_text)\n",
        "        text, text_moe_loss = self.moe_text(text)\n",
        "        residual_text = residual_text + text\n",
        "\n",
        "        # Total MoE auxiliary loss\n",
        "        moe_loss = video_moe_loss + text_moe_loss\n",
        "\n",
        "        return residual_video, residual_text, moe_loss\n",
        "\n",
        "\n",
        "# For Daily_Mail/CNN datasets\n",
        "class Model_MSMO(nn.Module):\n",
        "    def __init__(self, args=None):\n",
        "        super().__init__()\n",
        "        num_input_video = args.get('num_input_video')\n",
        "        num_input_text = args.get('num_input_text')\n",
        "        num_hidden = args.get('num_hidden')\n",
        "\n",
        "        self.ratio = args.get('ratio')\n",
        "\n",
        "        self.proj_fc_video = nn.Sequential(\n",
        "                                nn.Linear(num_input_video, num_hidden, bias=True),\n",
        "                                nn.Dropout(args.get('dropout_video')),\n",
        "                            )\n",
        "        self.proj_fc_text = nn.Sequential(\n",
        "                                nn.Linear(num_input_text, num_hidden, bias=True),\n",
        "                                nn.Dropout(args.get('dropout_text')),\n",
        "                            )\n",
        "\n",
        "        self.pos_embed_video = nn.Parameter(torch.zeros(1, 5000, num_hidden))\n",
        "        self.pos_embed_text = nn.Parameter(torch.zeros(1, 5000, num_hidden))\n",
        "        self.type_video = nn.Parameter(torch.zeros(1, 1, num_hidden))\n",
        "        self.type_text = nn.Parameter(torch.zeros(1, 1, num_hidden))\n",
        "        self.cls_token_video = nn.Parameter(torch.zeros(1, 1, num_hidden))\n",
        "        self.cls_token_text = nn.Parameter(torch.zeros(1, 1, num_hidden))\n",
        "\n",
        "        self.cls_mask_video = torch.ones([1, 1])\n",
        "        self.cls_mask_text = torch.ones([1, 1])\n",
        "\n",
        "        self.multiway_list = nn.ModuleList([MultiWayTransformer(num_hidden, dropout_attn=args.get('dropout_attn'))] * args.get('num_layers'))\n",
        "\n",
        "        self.norm_video = nn.LayerNorm(num_hidden)\n",
        "        self.norm_text = nn.LayerNorm(num_hidden)\n",
        "\n",
        "        self.fc_video = nn.Sequential(\n",
        "            nn.Linear(num_hidden, num_hidden),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(args.get('dropout_fc')),\n",
        "            nn.Linear(num_hidden, 1),\n",
        "        )\n",
        "        self.fc_text = nn.Sequential(\n",
        "            nn.Linear(num_hidden, num_hidden),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(args.get('dropout_fc')),\n",
        "            nn.Linear(num_hidden, 1),\n",
        "        )\n",
        "\n",
        "        self.num_layers = args.get('num_layers')\n",
        "\n",
        "        nn.init.trunc_normal_(self.pos_embed_video, std=.02)\n",
        "        nn.init.trunc_normal_(self.pos_embed_text, std=.02)\n",
        "        nn.init.trunc_normal_(self.type_video, std=.02)\n",
        "        nn.init.trunc_normal_(self.type_text, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token_video, std=.02)\n",
        "        nn.init.trunc_normal_(self.cls_token_text, std=.02)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def select_contrastive_embedding(self, score, embedding, mask, label):\n",
        "        B = score.shape[0]\n",
        "\n",
        "        key_embedding_list = []\n",
        "        nonkey_embedding_list = []\n",
        "        for i in range(B):\n",
        "            length = torch.sum(mask[i].to(torch.long))\n",
        "            key_embedding_num = max(1, length // self.ratio)\n",
        "            nonkey_embedding_num = max(1, length // self.ratio)\n",
        "\n",
        "            key_embedding_index = label[i].to(torch.bool)\n",
        "            key_embedding = embedding[i, key_embedding_index]\n",
        "\n",
        "            key_embedding_index_expand = ndimage.binary_dilation(label[i].cpu().detach().numpy(), iterations=4).astype(np.int32)\n",
        "            key_embedding_index_expand = torch.from_numpy(key_embedding_index_expand)\n",
        "\n",
        "            score_i = score[i, :length]\n",
        "            score_i = F.softmax(score_i, dim=-1)\n",
        "\n",
        "            _, idx_DESC = score_i.sort(descending=True)\n",
        "\n",
        "            non_key_embedding_index = []\n",
        "            for j in range(idx_DESC.shape[0]):\n",
        "                if key_embedding_index_expand[idx_DESC[j]] == 0:\n",
        "                    non_key_embedding_index.append(idx_DESC[j].item())\n",
        "                if len(non_key_embedding_index) >= nonkey_embedding_num:\n",
        "                    break\n",
        "\n",
        "            if len(non_key_embedding_index) == 0:\n",
        "                non_key_embedding_index.append(idx_DESC[-1])\n",
        "\n",
        "            nonkey_embedding = embedding[i, non_key_embedding_index]\n",
        "\n",
        "            key_embedding_list.append(key_embedding)\n",
        "            nonkey_embedding_list.append(nonkey_embedding)\n",
        "        return key_embedding_list, nonkey_embedding_list\n",
        "\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        video = kwargs['video']\n",
        "        text = kwargs['text']\n",
        "        mask_video = kwargs['mask_video']\n",
        "        mask_text = kwargs['mask_text']\n",
        "        video_label = kwargs['video_label']\n",
        "        text_label = kwargs['text_label']\n",
        "\n",
        "        B = video.shape[0]\n",
        "        video = self.proj_fc_video(video)\n",
        "        text = self.proj_fc_text(text)\n",
        "\n",
        "        # prepend the [CLSV] and [CLST] tokens to the video and text feature sequences\n",
        "        video = torch.cat([self.cls_token_video.expand(B, -1, -1), video], dim=1)\n",
        "        text = torch.cat([self.cls_token_text.expand(B, -1, -1), text], dim=1)\n",
        "        mask_video = torch.cat([self.cls_mask_video.expand(B, -1).to(mask_video), mask_video], dim=1)\n",
        "        mask_text = torch.cat([self.cls_mask_text.expand(B, -1).to(mask_text), mask_text], dim=1)\n",
        "\n",
        "        # add positional embedding\n",
        "        B, N_video, C = video.shape\n",
        "        B, N_text, C = text.shape\n",
        "        video = video + self.pos_embed_video[:, :N_video, :] + self.type_video\n",
        "        text = text + self.pos_embed_text[:, :N_text, :] + self.type_text\n",
        "        # Feature Concatenation (Early Fusion) :\n",
        "        fused = torch.cat([video, text], dim=1) # Text and video Are fused (concatenated in this cell) !!!!!\n",
        "        mask_fused = torch.cat([mask_video, mask_text], dim=1) #[B, N_video+N_text]\n",
        "        mask_fused = mask_fused.unsqueeze(1).expand(-1, N_video+N_text, -1) #[B, N_video+N_text, N_video+N_text]\n",
        "        # multiway transformer layers\n",
        "        total_moe_loss = 0\n",
        "        for i in range(self.num_layers):\n",
        "            video, text, moe_loss_layer = self.multiway_list[i](fused, mask_fused, N_video, N_text)\n",
        "            fused = torch.cat([video, text], dim=1)\n",
        "            total_moe_loss += moe_loss_layer  # Accumulate MoE loss\n",
        "\n",
        "\n",
        "        video = self.norm_video(video)\n",
        "        text = self.norm_text(text)\n",
        "\n",
        "        cls_video, video = torch.split(video, [1, N_video-1], dim=1)\n",
        "        cls_text, text = torch.split(text, [1, N_text-1], dim=1)\n",
        "\n",
        "        pred_video = self.fc_video(video).squeeze(-1) #[B, N]\n",
        "        pred_text = self.fc_text(text).squeeze(-1) #[B, N]\n",
        "\n",
        "        # select contrastive pairs for the intra-sample constrastive loss\n",
        "        key_video_list, nonkey_video_list = self.select_contrastive_embedding(pred_video, video, mask_video[:, 1:], video_label)\n",
        "        key_text_list, nonkey_text_list = self.select_contrastive_embedding(pred_text, text, mask_text[:, 1:], text_label)\n",
        "\n",
        "        contrastive_pairs = {\n",
        "            'key_video_list': key_video_list,\n",
        "            'nonkey_video_list': nonkey_video_list,\n",
        "            'key_text_list': key_text_list,\n",
        "            'nonkey_text_list': nonkey_text_list,\n",
        "            'cls_video': cls_video,\n",
        "            'cls_text': cls_text,\n",
        "        }\n",
        "\n",
        "        return pred_video, pred_text, contrastive_pairs, total_moe_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-VZ3iMxSoOB3",
      "metadata": {
        "id": "-VZ3iMxSoOB3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b145978-57e9-4347-ae81-b26542a7c2f5",
      "metadata": {
        "id": "5b145978-57e9-4347-ae81-b26542a7c2f5"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "def train_msmo(args):\n",
        "    batch_time = AverageMeter('time')\n",
        "    data_time = AverageMeter('time')\n",
        "\n",
        "    #if args.get('dataset') == 'BLiSS':\n",
        "     #   model = Model_BLiSS(args=args)\n",
        "    #elif args.get('dataset') in ['Daily_Mail', 'CNN']:\n",
        "    model = Model_MSMO(args=args)\n",
        "    # Lists to store loss values for plotting\n",
        "    tl_total = []\n",
        "    learning_Gap=[]\n",
        "    tl_text = []\n",
        "    tl_vid = []\n",
        "    tl_inter_c = []\n",
        "    tl_itra_c = []\n",
        "\n",
        "\n",
        "    model = model.to(args.get('device'))\n",
        "    calc_contrastive_loss = Dual_Contrastive_Loss().to(args.get('device'))\n",
        "\n",
        "    parameters = [p for p in model.parameters() if p.requires_grad] + \\\n",
        "                    [p for p in calc_contrastive_loss.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = torch.optim.Adam(parameters, lr=args.get('lr'), weight_decay=args.get('weight_decay'))\n",
        "\n",
        "    os.makedirs('{}/checkpoint'.format(args.get('model_dir')), exist_ok=True)\n",
        "\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True, split_summaries=True)\n",
        "\n",
        "    max_train_R1 = max_train_R2 = max_train_RL = max_train_cos = 0\n",
        "    max_val_R1 = max_val_R2 = max_val_RL = max_val_cos = 0\n",
        "    best_val_epoch = 0\n",
        "\n",
        "#    if args.get('dataset') in ['Daily_Mail', 'CNN']:\n",
        "#        dataset_name = 'MSMODataset'\n",
        "#    elif args.get('dataset') in ['BLiSS']:\n",
        "#        dataset_name = 'BLiSSDataset'\n",
        "\n",
        "    train_set = MSMODataset(mode='train', args=args)\n",
        "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=args.get('batch_size'), shuffle=True, num_workers=args.get('num_workers'),\n",
        "                                                drop_last=False, pin_memory=True,\n",
        "                                                worker_init_fn=worker_init_fn, collate_fn=my_collate_fn)\n",
        "    val_set = MSMODataset(mode='test', args=args)\n",
        "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=args.get('batch_size'), shuffle=False, num_workers=args.get('num_workers'),\n",
        "                                                drop_last=False, pin_memory=True,\n",
        "                                                worker_init_fn=worker_init_fn, collate_fn=my_collate_fn)\n",
        "\n",
        "    checkpoint_path = None\n",
        "    if args.get('checkpoint') and args.get('test'):\n",
        "        checkpoint_path = '{}/model_best_text.pt'.format(args.get('checkpoint'))\n",
        "        print(f\"load checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu',weights_only=False)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        val_R1, val_R2, val_RL, _ = evaluate_msmo(model, val_loader, args, epoch=0,rouge=rouge)\n",
        "\n",
        "        checkpoint_path = '{}/model_best_video.pt'.format(args.get('checkpoint'))\n",
        "        print(f\"load checkpoint from {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        _, _, _, val_cos = evaluate_msmo(model, val_loader, args, epoch=0,rouge=rouge)\n",
        "\n",
        "        print(f'R1: {val_R1:.4f} R2: {val_R2:.4f} RL: {val_RL:.4f} Cos: {val_cos:.4f}')\n",
        "        return val_R1, val_R2, val_RL, val_cos, best_val_epoch, max_train_R1, max_train_R2, max_train_RL, max_train_cos\n",
        "\n",
        "    print('\\n' + str(model))\n",
        "\n",
        "    for epoch in range(args.get('start_epoch'), args.get('max_epoch')):\n",
        "        model.train()\n",
        "        stats = AverageMeter('total_loss', 'text_loss', 'video_loss', 'inter_contrastive_loss', 'intra_contrastive_loss', 'R1', 'R2', 'RL', 'cos')\n",
        "\n",
        "        data_length = len(train_loader)\n",
        "        end = time.time()\n",
        "        for k, (video_list, video_summ_list, text_list, \\\n",
        "                mask_video_list, mask_video_summ_list, mask_text_list, \\\n",
        "                video_label_list, text_label_list, article_segment_list, highlight_list, \\\n",
        "                video_to_text_mask_list, text_to_video_mask_list) in enumerate(train_loader):\n",
        "            data_time.update(time=time.time() - end)\n",
        "\n",
        "            batch_size = len(video_list)\n",
        "\n",
        "            video = pad_sequence(video_list, batch_first=True)\n",
        "            video_summ = pad_sequence(video_summ_list, batch_first=True)\n",
        "            text = pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "            mask_video = pad_sequence(mask_video_list, batch_first=True)\n",
        "            mask_video_summ = pad_sequence(mask_video_summ_list, batch_first=True)\n",
        "            mask_text = pad_sequence(mask_text_list, batch_first=True)\n",
        "\n",
        "            video_label = pad_sequence(video_label_list, batch_first=True)\n",
        "            text_label = pad_sequence(text_label_list, batch_first=True)\n",
        "\n",
        "            for i in range(len(video_to_text_mask_list)):\n",
        "                video_to_text_mask_list[i] = video_to_text_mask_list[i].to(args.get('device'))\n",
        "                text_to_video_mask_list[i] = text_to_video_mask_list[i].to(args.get('device'))\n",
        "\n",
        "            video, video_summ, text = video.to(args.get('device')), video_summ.to(args.get('device')), text.to(args.get('device'))\n",
        "            mask_video, mask_video_summ, mask_text = mask_video.to(args.get('device')), mask_video_summ.to(args.get('device')), mask_text.to(args.get('device'))\n",
        "\n",
        "            video_label = video_label.to(args.get('device')) #[B, T]\n",
        "            text_label = text_label.to(args.get('device')) #[B, T]\n",
        "\n",
        "            pred_video, pred_text, contrastive_pairs,total_moe_loss = model(video=video, text=text, \\\n",
        "                                                                mask_video=mask_video, mask_text=mask_text, \\\n",
        "                                                                video_label=video_label, text_label=text_label, \\\n",
        "                                                                video_to_text_mask_list=video_to_text_mask_list, \\\n",
        "                                                                text_to_video_mask_list=text_to_video_mask_list)\n",
        "           # moe_loss = model.multiway_list[i].moe_loss  # Sum across layers\n",
        "            # Access the total MoE loss returned by the model forward pass.\n",
        "            moe_loss = total_moe_loss\n",
        "\n",
        "            num_frame_selected = torch.sum(video_label, dim=-1)\n",
        "            num_sentence_selected = torch.sum(text_label, dim=-1)\n",
        "            #print('predicted video :{}'.formatpred_video)\n",
        "            #print(pred_text)\n",
        "            mask_video_bool = mask_video.to(torch.bool)\n",
        "            mask_video_summ_bool = mask_video_summ.to(torch.bool)\n",
        "            mask_text_bool = mask_text.to(torch.bool)\n",
        "\n",
        "            # select frames and sentences with top-k highest importance score as predicted video and text summary\n",
        "            keyframe_index_list = []\n",
        "            keysentence_index_list = []\n",
        "            for i in range(batch_size):\n",
        "                keyframe_index_list.append(torch.topk(pred_video[i, mask_video_bool[i]], k=num_frame_selected[i])[1].tolist())\n",
        "                keysentence_index_list.append(torch.topk(pred_text[i, mask_text_bool[i]], k=num_sentence_selected[i])[1].tolist())\n",
        "\n",
        "            text_loss = calc_cls_loss(pred_text, text_label, mask=mask_text)\n",
        "            if args.get('dataset') in ['Daily_Mail', 'BLiSS']:\n",
        "                video_loss = calc_cls_loss(pred_video, video_label, mask=mask_video)\n",
        "            else:\n",
        "                video_loss = torch.zeros(1).to(text_loss)\n",
        "\n",
        "            inter_contrastive_loss, intra_contrastive_loss = calc_contrastive_loss(contrastive_pairs)\n",
        "\n",
        "            inter_contrastive_loss = inter_contrastive_loss * args.get('lambda_contrastive_inter')\n",
        "            intra_contrastive_loss = intra_contrastive_loss * args.get('lambda_contrastive_intra')\n",
        "            #loss = video_loss + text_loss + inter_contrastive_loss + intra_contrastive_loss\n",
        "                ###########################################################\n",
        "            # Add MoE loss here (new code)\n",
        "            total_loss = (\n",
        "                video_loss +\n",
        "                text_loss +\n",
        "                inter_contrastive_loss * args.get('lambda_contrastive_inter', 1.0) +\n",
        "                intra_contrastive_loss * args.get('lambda_contrastive_intra', 1.0) +\n",
        "                moe_loss * args.get('lambda_moe', 0.1)  # Scale MoE loss\n",
        "            )\n",
        "            ###########################################################\n",
        "\n",
        "\n",
        "#############################################################################################\"\"\n",
        "\n",
        "            if args.get('dataset') in ['Daily_Mail', 'BLiSS']:\n",
        "                video_cos = calc_video_cos(video, video_summ, keyframe_index_list, mask_video_summ=mask_video_summ_bool, dataset=args.get('dataset'))\n",
        "            else:\n",
        "                video_cos = 0\n",
        "            text_R1, text_R2, text_RL = calc_text_rouge(article_segment_list, highlight_list, keysentence_index_list, dataset=args.get('dataset'), rouge=rouge)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            stats.update(total_loss=total_loss.item(), text_loss=text_loss.item(), video_loss=video_loss.item(),\n",
        "                            inter_contrastive_loss=inter_contrastive_loss.item(), intra_contrastive_loss=intra_contrastive_loss.item(),\n",
        "                            R1=text_R1, R2=text_R2, RL=text_RL, cos=video_cos)\n",
        "\n",
        "            batch_time.update(time=time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            if (k + 1) % args.get('print_freq') == 0:\n",
        "                print(f'[Train] Epoch: {epoch+1}/{args[\"max_epoch\"]} Iter: {k+1}/{data_length} args[\"lr\"] '\n",
        "                            f'Time: {batch_time.time:.3f} Data: {data_time.time:.3f} '\n",
        "                            f'Loss: {stats.text_loss:.4f}/{stats.video_loss:.4f}/{stats.inter_contrastive_loss:.4f}/{stats.intra_contrastive_loss:.4f}/{stats.total_loss:.4f} '\n",
        "                            f'R1: {stats.R1:.4f} R2: {stats.R2:.4f} RL: {stats.RL:.4f} Cos: {stats.cos:.4f}')\n",
        "\n",
        "        max_train_R1 = max(stats.R1, max_train_R1)\n",
        "        max_train_R2 = max(stats.R2, max_train_R2)\n",
        "        max_train_RL = max(stats.RL, max_train_RL)\n",
        "        max_train_cos = max(stats.cos, max_train_cos)\n",
        "        tl_total.append(np.mean(stats.total_loss))  # Append training loss to list\n",
        "        tl_text.append(np.mean(stats.text_loss))\n",
        "        tl_vid.append(np.mean(stats.video_loss))\n",
        "        tl_inter_c.append(np.mean(stats.inter_contrastive_loss))\n",
        "        tl_itra_c.append(np.mean(stats.intra_contrastive_loss))\n",
        "\n",
        "        print(f'[Train] Epoch: {epoch+1}/{args[\"max_epoch\"]} '\n",
        "                    f'R1: {stats.R1:.4f}/{max_train_R1:.4f} '\n",
        "                    f'R2: {stats.R2:.4f}/{max_train_R2:.4f} '\n",
        "                    f'RL: {stats.RL:.4f}/{max_train_RL:.4f} '\n",
        "                    f'Cos: {stats.cos:.4f}/{max_train_cos:.4f}\\n'\n",
        "        )\n",
        "\n",
        "        args.get('writer').add_scalar(f'Train/max_train_R1', max_train_R1, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/max_train_R2', max_train_R2, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/max_train_RL', max_train_RL, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/max_train_cos', max_train_cos, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/train_R1', stats.R1, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/train_R2', stats.R2, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/train_RL', stats.RL, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/train_cos', stats.cos, epoch+1)\n",
        "\n",
        "        save_checkpoint = {\n",
        "            'epoch': epoch+1,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'max_val_R1': max_val_R1,\n",
        "            'max_val_R2': max_val_R2,\n",
        "            'max_val_RL': max_val_RL,\n",
        "            'max_val_cos': max_val_cos,\n",
        "        }\n",
        "\n",
        "        if (epoch + 1) % args.get('eval_freq') == 0:\n",
        "            val_R1, val_R2, val_RL, val_cos,vl_inter_c,vl_itra_c,vl_text,vl_vid,vl_total = evaluate_msmo(model, val_loader, args, epoch=epoch,rouge=rouge)\n",
        "            max_val_R2 = max(val_R2, max_val_R2)\n",
        "            max_val_RL = max(val_RL, max_val_RL)\n",
        "            if max_val_R1 < val_R1:\n",
        "                max_val_R1 = max(val_R1, max_val_R1)\n",
        "                best_val_epoch = epoch + 1\n",
        "                torch.save(save_checkpoint, '{}/checkpoint/model_best_text.pt'.format(args.get('model_dir')))\n",
        "            if max_val_cos < val_cos:\n",
        "                max_val_cos = max(val_cos, max_val_cos)\n",
        "                torch.save(save_checkpoint, '{}/checkpoint/model_best_video.pt'.format(args.get('model_dir')))\n",
        "\n",
        "            print(f'[Eval]  Epoch: {epoch+1}/{args[\"max_epoch\"]} '\n",
        "                        f'R1: {val_R1:.4f}/{max_val_R1:.4f} '\n",
        "                        f'R2: {val_R2:.4f}/{max_val_R2:.4f} '\n",
        "                        f'RL: {val_RL:.4f}/{max_val_RL:.4f} '\n",
        "                        f'Cos: {val_cos:.4f}/{max_val_cos:.4f}\\n\\n'\n",
        "            )\n",
        "\n",
        "            args.get('writer').add_scalar(f'Val/max_val_R1', max_val_R1, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/max_val_R2', max_val_R2, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/max_val_RL', max_val_RL, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/max_val_cos', max_val_cos, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/val_R1', val_R1, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/val_R2', val_R2, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/val_RL', val_RL, epoch+1)\n",
        "            args.get('writer').add_scalar(f'Val/val_cos', val_cos, epoch+1)\n",
        "\n",
        "        args.get('writer').add_scalar(f'Train/loss', stats.total_loss, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/text_loss', stats.text_loss, epoch+1)\n",
        "        args.get('writer').add_scalar(f'Train/video_loss', stats.video_loss, epoch+1)\n",
        "\n",
        "    return max_val_R1, max_val_R2, max_val_RL, max_val_cos, best_val_epoch, \\\n",
        "            max_train_R1, max_train_R2, max_train_RL, max_train_cos,tl_total,tl_text,tl_vid,tl_inter_c,tl_itra_c,vl_inter_c,vl_itra_c,vl_text,vl_vid,vl_total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_msmo(model, val_loader, args, epoch=None, mode='train',rouge=None):\n",
        "    stats = AverageMeter('R1', 'R2', 'RL', 'cos')\n",
        "    data_length = len(val_loader)\n",
        "    calc_contrastive_loss = Dual_Contrastive_Loss().to(args.get('device'))\n",
        "    model.eval()\n",
        "    vl_total = []\n",
        "    vl_text = []\n",
        "    vl_vid = []\n",
        "    vl_inter_c = []\n",
        "    vl_itra_c = []\n",
        "    for k, (video_list, video_summ_list, text_list, \\\n",
        "            mask_video_list, mask_video_summ_list, mask_text_list, \\\n",
        "            video_label_list, text_label_list, article_segment_list, highlight_list, \\\n",
        "            video_to_text_mask_list, text_to_video_mask_list) in enumerate(val_loader):\n",
        "\n",
        "        batch_size = len(video_list)\n",
        "\n",
        "        video = pad_sequence(video_list, batch_first=True)\n",
        "        video_summ = pad_sequence(video_summ_list, batch_first=True)\n",
        "        text = pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "        mask_video = pad_sequence(mask_video_list, batch_first=True)\n",
        "        mask_video_summ = pad_sequence(mask_video_summ_list, batch_first=True)\n",
        "        mask_text = pad_sequence(mask_text_list, batch_first=True)\n",
        "\n",
        "        video_label = pad_sequence(video_label_list, batch_first=True)\n",
        "        text_label = pad_sequence(text_label_list, batch_first=True)\n",
        "\n",
        "        video, video_summ, text = video.to(args.get('device')), video_summ.to(args.get('device')), text.to(args.get('device'))\n",
        "        mask_video, mask_video_summ, mask_text = mask_video.to(args.get('device')), mask_video_summ.to(args.get('device')), mask_text.to(args.get('device'))\n",
        "\n",
        "        video_label = video_label.to(args.get('device')) #[B, T]\n",
        "        text_label = text_label.to(args.get('device')) #[B, T]\n",
        "\n",
        "        for i in range(len(video_to_text_mask_list)):\n",
        "            video_to_text_mask_list[i] = video_to_text_mask_list[i].to(args.get('device'))\n",
        "            text_to_video_mask_list[i] = text_to_video_mask_list[i].to(args.get('device'))\n",
        "\n",
        "        pred_video, pred_text, contrastive_pairs,moe_loss = model(video=video, text=text, \\\n",
        "                                                            mask_video=mask_video, mask_text=mask_text, \\\n",
        "                                                            video_label=video_label, text_label=text_label, \\\n",
        "                                                            video_to_text_mask_list=video_to_text_mask_list, \\\n",
        "                                                            text_to_video_mask_list=text_to_video_mask_list)\n",
        "\n",
        "        num_frame_selected = torch.sum(video_label, dim=-1)\n",
        "        num_sentence_selected = torch.sum(text_label, dim=-1)\n",
        "        #print('predicted text ; {}'.format(pred_text))\n",
        "        #print('predicted video ; {}'.format(pred_video))\n",
        "        mask_video_bool = mask_video.to(torch.bool)\n",
        "        mask_video_summ_bool = mask_video_summ.to(torch.bool)\n",
        "        mask_text_bool = mask_text.to(torch.bool)\n",
        "        keyframe_index_list = []\n",
        "        keysentence_index_list = []\n",
        "        for i in range(batch_size):\n",
        "            keyframe_index_list.append(torch.topk(pred_video[i, mask_video_bool[i]], k=num_frame_selected[i])[1].tolist())\n",
        "            keysentence_index_list.append(torch.topk(pred_text[i, mask_text_bool[i]], k=num_sentence_selected[i])[1].tolist())\n",
        "\n",
        "        if args.get('dataset') in ['Daily_Mail', 'BLiSS']:\n",
        "            video_cos = calc_video_cos(video, video_summ, keyframe_index_list, mask_video_summ=mask_video_summ_bool, dataset=args.get('dataset'))\n",
        "        else:\n",
        "            video_cos = 0\n",
        "        text_R1, text_R2, text_RL = calc_text_rouge(article_segment_list, highlight_list, keysentence_index_list, dataset=args.get('dataset'), rouge=rouge)\n",
        "       # print('\\nhighlight_list : \\n{}'.format(highlight_list))\n",
        "        #print('\\nkeysentence_index_list : \\n{}'.format(keysentence_index_list))\n",
        "        #print('\\article_segment_list : \\n{}'.format(article_segment_list))\n",
        "        stats.update(R1=text_R1, R2=text_R2, RL=text_RL, cos=video_cos)\n",
        "        ## Losses :\n",
        "        inter_contrastive_loss, intra_contrastive_loss = calc_contrastive_loss(contrastive_pairs)\n",
        "\n",
        "        inter_contrastive_loss = inter_contrastive_loss * args.get('lambda_contrastive_inter')\n",
        "        intra_contrastive_loss = intra_contrastive_loss * args.get('lambda_contrastive_intra')\n",
        "        video_loss = calc_cls_loss(pred_video, video_label, mask=mask_video)\n",
        "        text_loss = calc_cls_loss(pred_text, text_label, mask=mask_text)\n",
        "        vl_inter_c.append(inter_contrastive_loss.item())\n",
        "        vl_itra_c.append(intra_contrastive_loss.item())\n",
        "        vl_text.append(text_loss.item())\n",
        "        vl_vid.append(video_loss.item())\n",
        "        vl_total.append(video_loss.item()+text_loss.item()+inter_contrastive_loss.item()+intra_contrastive_loss.item())\n",
        "        if (k + 1) % args.get('print_freq') == 0:\n",
        "            print(f'[Eval]  Epoch: {epoch+1}/{args[\"max_epoch\"]} Iter: {k+1}/{data_length} '\n",
        "                        f'R1: {stats.R1:.4f} R2: {stats.R2:.4f} RL: {stats.RL:.4f} Cos: {stats.cos:.4f}')\n",
        "    return stats.R1, stats.R2, stats.RL, stats.cos,vl_inter_c,vl_itra_c,vl_text,vl_vid,vl_total\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_xla[tpu] -f https://storage.googleapis.com/tpu-pytorch/wheels/colab.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0StdN4VilX8R",
        "outputId": "3d290b3f-eb18-497d-946f-614cec9dcbfc"
      },
      "id": "0StdN4VilX8R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/tpu-pytorch/wheels/colab.html\n",
            "Requirement already satisfied: torch_xla[tpu] in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (2.32.3)\n",
            "Requirement already satisfied: libtpu-nightly==0.1.dev20240916 in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (0.1.dev20240916+nightly)\n",
            "Requirement already satisfied: tpu-info in /usr/local/lib/python3.11/dist-packages (from torch_xla[tpu]) (0.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla[tpu]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla[tpu]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla[tpu]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_xla[tpu]) (2025.1.31)\n",
            "Requirement already satisfied: grpcio>=1.65.5 in /usr/local/lib/python3.11/dist-packages (from tpu-info->torch_xla[tpu]) (1.70.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from tpu-info->torch_xla[tpu]) (5.29.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from tpu-info->torch_xla[tpu]) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->tpu-info->torch_xla[tpu]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->tpu-info->torch_xla[tpu]) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->tpu-info->torch_xla[tpu]) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JofA6VVAlcmu",
        "outputId": "c98fb6b8-d944-4a15-bdfd-f83d732f46f3"
      },
      "id": "JofA6VVAlcmu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1e08913-c968-457d-94ff-4faf4f0c4583",
      "metadata": {
        "id": "f1e08913-c968-457d-94ff-4faf4f0c4583"
      },
      "outputs": [],
      "source": [
        "args ={ \"dataset\" : 'Daily_Mail',\n",
        "        \"data_root\":'/content/drive/MyDrive/data' ,\n",
        "       \"device\" : 'xla',\n",
        "       \"start_epoch\":0,\n",
        "       \"num_workers\" :2,\n",
        "       \"model_dir\" : 'logsV4',\n",
        "       \"log_file\" : 'logV1.txt',\n",
        "       \"nms_thresh\": 0.4,\n",
        "       \"print_freq\":5,\n",
        "       \"eval_freq\":1,\n",
        "       #\"checkpoint\":'/media/pc/New Volume/Saad/Models/A2Summ/logs/checkpointV1',\n",
        "       \"test\" :False,\n",
        "       \"num_feature\":512,\n",
        "        \"lr\":2e-4,\n",
        "        \"lambda_moe\": 0.4,        # MoE loss weight\n",
        "        \"num_experts\": 5,         # Number of experts per MoE layer\n",
        "        \"moe_dropout\": 0.05,\n",
        "        \"weight_decay\": 1e-7,\n",
        "        \"max_epoch\": 1,\n",
        "        \"batch_size\": 4,\n",
        "        \"seed\": 12345,\n",
        "\n",
        "        \"num_input_video\": 2048,\n",
        "        \"num_input_text\": 768,\n",
        "        \"num_hidden\": 256,\n",
        "        \"num_layers\": 2,\n",
        "\n",
        "        \"dropout_video\": 0.1,\n",
        "        \"dropout_text\": 0.1,\n",
        "        \"dropout_attn\": 0.1,\n",
        "        \"dropout_fc\": 0.5,\n",
        "\n",
        "        \"lambda_contrastive_inter\": 0.001,\n",
        "        \"lambda_contrastive_intra\": 0.001,\n",
        "        \"ratio\": 8,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "init_logger(args[\"model_dir\"], args.get('log_file'))\n",
        "set_random_seed(args.get('seed'))\n",
        "dump_yaml(args, '{}/args.yml'.format(args[\"model_dir\"]))\n",
        "\n",
        "logger.info(args)\n",
        "os.makedirs(args.get('model_dir'), exist_ok=True)\n",
        "print(args.get('model_dir'))\n",
        "\n",
        "args[\"writer\"] = SummaryWriter(os.path.join(args[\"model_dir\"], 'tensorboard'))\n",
        "\n",
        "max_val_R1, max_val_R2, max_val_RL, max_val_cos, best_val_epoch, \\\n",
        "    max_train_R1, max_train_R2, max_train_RL, max_train_cos = train_msmo(args)\n",
        "\n",
        "logger.info(f'Training done. Val R1: {max_val_R1:.4f}, R2: {max_val_R2:.4f}, RL: {max_val_RL:.4f}, Cos: {max_val_cos:.4f}, Best Epoch:{best_val_epoch}.')\n",
        "logger.info(f'             Train R1: {max_train_R1:.4f}, R2: {max_train_R2:.4f}, RL: {max_train_RL:.4f}, Cos: {max_train_cos:.4f}.\\n\\n')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cmj1pUbUbhu_",
        "outputId": "4898662e-e992-46f2-baa8-e016bcc2ada1"
      },
      "id": "cmj1pUbUbhu_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logsV4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1207/1207 [00:02<00:00, 415.65it/s]\n",
            "100%|██████████| 352/352 [00:00<00:00, 574.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model_MSMO(\n",
            "  (proj_fc_video): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (proj_fc_text): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (multiway_list): ModuleList(\n",
            "    (0-1): 2 x MultiWayTransformer(\n",
            "      (norm1_fused): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_fusion): MultiHeadAttention(q_dims=256, k_dims=256, v_dims=256, h_dims=256, o_dims=256, heads=8, p=0.1, bias=True)\n",
            "      (norm2_video): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (moe_video): MoELayer(\n",
            "        (experts): ModuleList(\n",
            "          (0-3): 4 x FFN(\n",
            "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.1, inplace=False)\n",
            "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop2): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (gate): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "      (norm2_text): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (moe_text): MoELayer(\n",
            "        (experts): ModuleList(\n",
            "          (0-3): 4 x FFN(\n",
            "            (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "            (act): GELU(approximate='none')\n",
            "            (drop1): Dropout(p=0.1, inplace=False)\n",
            "            (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "            (drop2): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (gate): Linear(in_features=256, out_features=4, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm_video): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm_text): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc_video): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc_text): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6e0ea80f9873>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmax_val_R1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_R2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_RL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_cos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmax_train_R1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_R2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_RL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_cos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_msmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training done. Val R1: {max_val_R1:.4f}, R2: {max_val_R2:.4f}, RL: {max_val_RL:.4f}, Cos: {max_val_cos:.4f}, Best Epoch:{best_val_epoch}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-11d6886880e9>\u001b[0m in \u001b[0;36mtrain_msmo\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mtext_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#[B, T]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             pred_video, pred_text, contrastive_pairs,total_moe_loss = model(video=video, text=text, \\\n\u001b[0m\u001b[1;32m    118\u001b[0m                                                                 \u001b[0mmask_video\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                                                                 \u001b[0mvideo_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-17f44cf40f17>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# select contrastive pairs for the intra-sample constrastive loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mkey_video_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonkey_video_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_contrastive_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_video\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_video\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mkey_text_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonkey_text_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_contrastive_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-17f44cf40f17>\u001b[0m in \u001b[0;36mselect_contrastive_embedding\u001b[0;34m(self, score, embedding, mask, label)\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_DESC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mkey_embedding_index_expand\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_DESC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0mnon_key_embedding_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx_DESC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnon_key_embedding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnonkey_embedding_num\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9da28c12-1bc4-4169-9013-8ed1b618a295",
      "metadata": {
        "id": "9da28c12-1bc4-4169-9013-8ed1b618a295"
      },
      "outputs": [],
      "source": [
        "args ={ \"dataset\" : 'Daily_Mail',\n",
        "        \"data_root\":'/content/drive/MyDrive/data' ,\n",
        "       \"device\" : 'cpu',\n",
        "       \"start_epoch\":0,\n",
        "       \"num_workers\" :2,\n",
        "       \"model_dir\" : '/content/logsV3',\n",
        "       \"log_file\" : 'logV3.txt',\n",
        "       \"nms_thresh\": 0.4,\n",
        "       \"print_freq\":5,\n",
        "       \"eval_freq\":1,\n",
        "       \"checkpoint\":'/content/logsV3/checkpoint',\n",
        "       \"test\" :True,\n",
        "       \"num_feature\":512,\n",
        "        \"lr\":2e-4,\n",
        "        \"weight_decay\": 1e-7,\n",
        "        \"max_epoch\": 1,\n",
        "        \"batch_size\": 4,\n",
        "        \"seed\": 12345,\n",
        "\n",
        "        \"num_input_video\": 2048,\n",
        "        \"num_input_text\": 768,\n",
        "        \"num_hidden\": 256,\n",
        "        \"num_layers\": 2,\n",
        "\n",
        "        \"dropout_video\": 0.1,\n",
        "        \"dropout_text\": 0.1,\n",
        "        \"dropout_attn\": 0.1,\n",
        "        \"dropout_fc\": 0.5,\n",
        "\n",
        "        \"lambda_contrastive_inter\": 0.001,\n",
        "        \"lambda_contrastive_intra\": 0.001,\n",
        "        \"ratio\": 8,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feddaad7-c3d6-4ef9-91d6-4b9a77e913a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "feddaad7-c3d6-4ef9-91d6-4b9a77e913a6",
        "outputId": "31e884f7-a574-4e48-fbe0-33d87b05080f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/logsV3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1207/1207 [00:00<00:00, 6058.72it/s]\n",
            "100%|██████████| 352/352 [00:00<00:00, 6009.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load checkpoint from /content/logsV3/checkpoint/model_best_text.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/logsV3/checkpoint/model_best_text.pt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-6e0ea80f9873>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmax_val_R1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_R2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_RL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_val_cos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_val_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmax_train_R1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_R2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_RL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_cos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_msmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training done. Val R1: {max_val_R1:.4f}, R2: {max_val_R2:.4f}, RL: {max_val_RL:.4f}, Cos: {max_val_cos:.4f}, Best Epoch:{best_val_epoch}.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a0633b102d14>\u001b[0m in \u001b[0;36mtrain_msmo\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}/model_best_text.pt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoint'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"load checkpoint from {checkpoint_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mval_R1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_R2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_RL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_msmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrouge\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrouge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/logsV3/checkpoint/model_best_text.pt'"
          ]
        }
      ],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "init_logger(args[\"model_dir\"], args.get('log_file'))\n",
        "set_random_seed(args.get('seed'))\n",
        "dump_yaml(args, '{}/args.yml'.format(args[\"model_dir\"]))\n",
        "\n",
        "logger.info(args)\n",
        "os.makedirs(args.get('model_dir'), exist_ok=True)\n",
        "print(args.get('model_dir'))\n",
        "\n",
        "args[\"writer\"] = SummaryWriter(os.path.join(args[\"model_dir\"], 'tensorboard'))\n",
        "\n",
        "max_val_R1, max_val_R2, max_val_RL, max_val_cos, best_val_epoch, \\\n",
        "    max_train_R1, max_train_R2, max_train_RL, max_train_cos = train_msmo(args)\n",
        "\n",
        "logger.info(f'Training done. Val R1: {max_val_R1:.4f}, R2: {max_val_R2:.4f}, RL: {max_val_RL:.4f}, Cos: {max_val_cos:.4f}, Best Epoch:{best_val_epoch}.')\n",
        "logger.info(f'             Train R1: {max_train_R1:.4f}, R2: {max_train_R2:.4f}, RL: {max_train_RL:.4f}, Cos: {max_train_cos:.4f}.\\n\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e888c35b-a31d-4bab-8e7d-818fd8bf6b92",
      "metadata": {
        "id": "e888c35b-a31d-4bab-8e7d-818fd8bf6b92"
      },
      "outputs": [],
      "source": [
        "model_txt=Model_MSMO(args)\n",
        "model_vid=Model_MSMO(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d047b6-8170-473a-a1e9-bb31dbf4e9de",
      "metadata": {
        "id": "27d047b6-8170-473a-a1e9-bb31dbf4e9de",
        "outputId": "96365e5c-f0ed-40e2-d0d2-1015be5a4e13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "load checkpoint from /media/pc/New Volume/Saad/Models/A2Summ/logs/checkpoint/model_best_text.pt\n",
            "load checkpoint from /media/pc/New Volume/Saad/Models/A2Summ/logs/checkpoint/model_best_video.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_7411/4193949215.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
            "/tmp/ipykernel_7411/4193949215.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location='cpu')\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_path = '{}/model_best_text.pt'.format(args.get('checkpoint'))\n",
        "print(f\"load checkpoint from {checkpoint_path}\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "model_txt.load_state_dict(checkpoint['model_state_dict'])\n",
        "#val_R1, val_R2, val_RL, _ = evaluate_msmo(model, val_loader, args, epoch=0)\n",
        "\n",
        "checkpoint_path = '{}/model_best_video.pt'.format(args.get('checkpoint'))\n",
        "print(f\"load checkpoint from {checkpoint_path}\")\n",
        "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "model_vid.load_state_dict(checkpoint['model_state_dict'])\n",
        "#_, _, _, val_cos = evaluate_msmo(model, val_loader, args, epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bc7e49b-4e41-4e5f-be27-b6d1ac1ad087",
      "metadata": {
        "id": "9bc7e49b-4e41-4e5f-be27-b6d1ac1ad087",
        "outputId": "06efbd95-c994-4b51-e156-a97cfc79a1ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_MSMO(\n",
            "  (proj_fc_video): Sequential(\n",
            "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (proj_fc_text): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
            "    (1): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (multiway_list): ModuleList(\n",
            "    (0-1): 2 x MultiWayTransformer(\n",
            "      (norm1_fused): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (attn_fusion): MultiHeadAttention(q_dims=256, k_dims=256, v_dims=256, h_dims=256, o_dims=256, heads=8, p=0.1, bias=True)\n",
            "      (norm2_video): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn_video): FFN(\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (norm2_text): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      (ffn_text): FFN(\n",
            "        (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
            "        (act): GELU(approximate='none')\n",
            "        (drop1): Dropout(p=0.1, inplace=False)\n",
            "        (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "        (drop2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (norm_video): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm_text): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc_video): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            "  (fc_text): Sequential(\n",
            "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49a8c99-c0a3-42fc-bd7f-96ebe80f7c9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49a8c99-c0a3-42fc-bd7f-96ebe80f7c9b",
        "outputId": "02249bc8-90cb-4cfc-9cdb-e6c8a722af95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: torch 2.5.1+cpu\n",
            "Uninstalling torch-2.5.1+cpu:\n",
            "  Successfully uninstalled torch-2.5.1+cpu\n",
            "Found existing installation: torchvision 0.20.1+cpu\n",
            "Uninstalling torchvision-0.20.1+cpu:\n",
            "  Successfully uninstalled torchvision-0.20.1+cpu\n",
            "Found existing installation: torchaudio 2.5.1+cpu\n",
            "Uninstalling torchaudio-2.5.1+cpu:\n",
            "  Successfully uninstalled torchaudio-2.5.1+cpu\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.21.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118 torchaudio-2.6.0+cu118 torchvision-0.21.0+cu118 triton-3.2.0\n"
          ]
        }
      ],
      "source": [
        "# Uninstall the current PyTorch installation (if any).\n",
        "!pip uninstall -y torch torchvision torchaudio\n",
        "\n",
        "# Install PyTorch with CUDA support.\n",
        "# Make sure to select the correct CUDA version for your environment.\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Restart the runtime. This is important for the changes to take effect.\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UnYHd71yqw2t",
      "metadata": {
        "id": "UnYHd71yqw2t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}